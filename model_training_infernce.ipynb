{
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "!pip install CatBoost\n",
        "!pip install scikeras"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hixmCbz-mOJ9",
        "outputId": "cc31fc07-e78b-4d2c-d49a-f0719b07371f"
      },
      "id": "hixmCbz-mOJ9",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: CatBoost in /usr/local/lib/python3.12/dist-packages (1.2.8)\n",
            "Requirement already satisfied: graphviz in /usr/local/lib/python3.12/dist-packages (from CatBoost) (0.21)\n",
            "Requirement already satisfied: matplotlib in /usr/local/lib/python3.12/dist-packages (from CatBoost) (3.10.0)\n",
            "Requirement already satisfied: numpy<3.0,>=1.16.0 in /usr/local/lib/python3.12/dist-packages (from CatBoost) (2.0.2)\n",
            "Requirement already satisfied: pandas>=0.24 in /usr/local/lib/python3.12/dist-packages (from CatBoost) (2.2.2)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.12/dist-packages (from CatBoost) (1.16.3)\n",
            "Requirement already satisfied: plotly in /usr/local/lib/python3.12/dist-packages (from CatBoost) (5.24.1)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.12/dist-packages (from CatBoost) (1.17.0)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.12/dist-packages (from pandas>=0.24->CatBoost) (2.9.0.post0)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.12/dist-packages (from pandas>=0.24->CatBoost) (2025.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.12/dist-packages (from pandas>=0.24->CatBoost) (2025.2)\n",
            "Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.12/dist-packages (from matplotlib->CatBoost) (1.3.3)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.12/dist-packages (from matplotlib->CatBoost) (0.12.1)\n",
            "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.12/dist-packages (from matplotlib->CatBoost) (4.60.1)\n",
            "Requirement already satisfied: kiwisolver>=1.3.1 in /usr/local/lib/python3.12/dist-packages (from matplotlib->CatBoost) (1.4.9)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.12/dist-packages (from matplotlib->CatBoost) (25.0)\n",
            "Requirement already satisfied: pillow>=8 in /usr/local/lib/python3.12/dist-packages (from matplotlib->CatBoost) (11.3.0)\n",
            "Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.12/dist-packages (from matplotlib->CatBoost) (3.2.5)\n",
            "Requirement already satisfied: tenacity>=6.2.0 in /usr/local/lib/python3.12/dist-packages (from plotly->CatBoost) (8.5.0)\n",
            "Requirement already satisfied: scikeras in /usr/local/lib/python3.12/dist-packages (0.13.0)\n",
            "Requirement already satisfied: keras>=3.2.0 in /usr/local/lib/python3.12/dist-packages (from scikeras) (3.10.0)\n",
            "Requirement already satisfied: scikit-learn>=1.4.2 in /usr/local/lib/python3.12/dist-packages (from scikeras) (1.6.1)\n",
            "Requirement already satisfied: absl-py in /usr/local/lib/python3.12/dist-packages (from keras>=3.2.0->scikeras) (1.4.0)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.12/dist-packages (from keras>=3.2.0->scikeras) (2.0.2)\n",
            "Requirement already satisfied: rich in /usr/local/lib/python3.12/dist-packages (from keras>=3.2.0->scikeras) (13.9.4)\n",
            "Requirement already satisfied: namex in /usr/local/lib/python3.12/dist-packages (from keras>=3.2.0->scikeras) (0.1.0)\n",
            "Requirement already satisfied: h5py in /usr/local/lib/python3.12/dist-packages (from keras>=3.2.0->scikeras) (3.15.1)\n",
            "Requirement already satisfied: optree in /usr/local/lib/python3.12/dist-packages (from keras>=3.2.0->scikeras) (0.18.0)\n",
            "Requirement already satisfied: ml-dtypes in /usr/local/lib/python3.12/dist-packages (from keras>=3.2.0->scikeras) (0.5.4)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.12/dist-packages (from keras>=3.2.0->scikeras) (25.0)\n",
            "Requirement already satisfied: scipy>=1.6.0 in /usr/local/lib/python3.12/dist-packages (from scikit-learn>=1.4.2->scikeras) (1.16.3)\n",
            "Requirement already satisfied: joblib>=1.2.0 in /usr/local/lib/python3.12/dist-packages (from scikit-learn>=1.4.2->scikeras) (1.5.2)\n",
            "Requirement already satisfied: threadpoolctl>=3.1.0 in /usr/local/lib/python3.12/dist-packages (from scikit-learn>=1.4.2->scikeras) (3.6.0)\n",
            "Requirement already satisfied: typing-extensions>=4.6.0 in /usr/local/lib/python3.12/dist-packages (from optree->keras>=3.2.0->scikeras) (4.15.0)\n",
            "Requirement already satisfied: markdown-it-py>=2.2.0 in /usr/local/lib/python3.12/dist-packages (from rich->keras>=3.2.0->scikeras) (4.0.0)\n",
            "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /usr/local/lib/python3.12/dist-packages (from rich->keras>=3.2.0->scikeras) (2.19.2)\n",
            "Requirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.12/dist-packages (from markdown-it-py>=2.2.0->rich->keras>=3.2.0->scikeras) (0.1.2)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "8a689ddb",
      "metadata": {
        "id": "8a689ddb"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "\n",
        "from sklearn.model_selection import train_test_split, StratifiedKFold, GridSearchCV\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.metrics import accuracy_score, classification_report\n",
        "from sklearn.metrics import ConfusionMatrixDisplay\n",
        "\n",
        "from sklearn.pipeline import Pipeline\n",
        "\n",
        "from sklearn.naive_bayes import GaussianNB\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.svm import LinearSVC, SVC\n",
        "\n",
        "# New Imports required for the new models\n",
        "from sklearn.cluster import KMeans\n",
        "from sklearn.mixture import GaussianMixture as GMM\n",
        "\n",
        "# Ensemble Models\n",
        "from xgboost import XGBClassifier\n",
        "from lightgbm import LGBMClassifier\n",
        "from catboost import CatBoostClassifier\n",
        "from sklearn.impute import SimpleImputer\n",
        "\n",
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Dense, Dropout\n",
        "from scikeras.wrappers import KerasClassifier # To use Keras with GridSearchCV/Scikit-learn\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "from keras.utils import to_categorical\n",
        "\n",
        "import matplotlib.pyplot as plt"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "41c44e53",
      "metadata": {
        "id": "41c44e53"
      },
      "source": [
        "Load processed data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "ae7efd6b",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ae7efd6b",
        "outputId": "fd3d949d-6605-41b8-ff5f-c2a6c00d954e"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Data Shape: (12620, 82)\n",
            "Test Data Shape: (5852, 81)\n"
          ]
        }
      ],
      "source": [
        "# Processed data (one-hot / True-False etc.)\n",
        "train_df = pd.read_csv(\"train_processed.csv\")\n",
        "test_df = pd.read_csv(\"test_processed.csv\")\n",
        "\n",
        "# Assuming 'test_raw.csv' was used to get the original trip_id column\n",
        "# If not available, replace 'test_raw.csv' with the actual filename containing 'trip_id'\n",
        "try:\n",
        "    test_raw = pd.read_csv('test_raw.csv')\n",
        "except FileNotFoundError:\n",
        "    print(\"Warning: 'test_raw.csv' not found. Assuming 'test_processed.csv' contains 'trip_id'.\")\n",
        "    test_raw = test_df.copy()\n",
        "\n",
        "print(f\"Train Data Shape: {train_df.shape}\")\n",
        "print(f\"Test Data Shape: {test_df.shape}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "aacb8afc",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "aacb8afc",
        "outputId": "c3b8e280-b993-400f-8d83-3d28dc4cd0f6"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Feature names cleaned for LightGBM compatibility.\n"
          ]
        }
      ],
      "source": [
        "# --- FIX: Clean column names for LightGBM compatibility ---\n",
        "import re\n",
        "\n",
        "def clean_feature_names(df):\n",
        "    \"\"\"\n",
        "    Cleans column names by replacing problematic characters (commas, brackets,\n",
        "    and other special symbols) with underscores for LightGBM compatibility.\n",
        "    \"\"\"\n",
        "    new_cols = []\n",
        "    for col in df.columns:\n",
        "        # 1. Replace commas and brackets with underscores\n",
        "        cleaned_col = col.replace(',', '_').replace('[', '_').replace(']', '_')\n",
        "        # 2. Replace any remaining non-alphanumeric/non-underscore sequence with a single underscore\n",
        "        cleaned_col = re.sub(r'[^A-Za-z0-9_]+', '_', cleaned_col)\n",
        "        # 3. Clean up leading/trailing underscores and double underscores\n",
        "        cleaned_col = cleaned_col.strip('_').replace('__', '_')\n",
        "        new_cols.append(cleaned_col)\n",
        "\n",
        "    df.columns = new_cols\n",
        "    return df\n",
        "\n",
        "# Apply the cleaning to both dataframes before splitting\n",
        "train_df = clean_feature_names(train_df)\n",
        "test_df = clean_feature_names(test_df)\n",
        "\n",
        "print(\"Feature names cleaned for LightGBM compatibility.\")\n",
        "# -----------------------------------------------------------"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Evaluate Model on Training Data"
      ],
      "metadata": {
        "id": "sfglBEpFxcRt"
      },
      "id": "sfglBEpFxcRt"
    },
    {
      "cell_type": "code",
      "source": [
        "def evaluate_model(model, X, y, model_name):\n",
        "    y_pred = model.predict(X)\n",
        "    accuracy = accuracy_score(y, y_pred)\n",
        "    print(f\"\\n--- {model_name} ---\")\n",
        "    print(f\"Accuracy: {accuracy:.4f}\")\n",
        "    print(\"Classification Report:\\n\", classification_report(y, y_pred, zero_division=0))\n",
        "\n",
        "    # Optional: Display confusion matrix\n",
        "    # disp = ConfusionMatrixDisplay.from_estimator(model, X, y, normalize='true')\n",
        "    # disp.ax_.set_title(f\"Confusion Matrix for {model_name}\")\n",
        "    # plt.show()\n",
        "\n",
        "    return y_pred"
      ],
      "metadata": {
        "id": "MSqSrY6HxjRt"
      },
      "id": "MSqSrY6HxjRt",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "id": "36d0cb00",
      "metadata": {
        "id": "36d0cb00"
      },
      "source": [
        "Cross-Validation Split"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "ecd27abd",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ecd27abd",
        "outputId": "b2d81177-9a77-4bc9-9e5d-e6e3ba5950e6"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "X_train shape: (12620, 81)\n",
            "y_train shape: (12620,)\n",
            "X_test shape: (5852, 81)\n",
            "Keras target prepared. Input dimension: 81, Output classes: 3\n",
            "Number of target classes: 3\n"
          ]
        }
      ],
      "source": [
        "TARGET = 'spend_category'\n",
        "\n",
        "X_train = train_df.drop(columns=[TARGET])\n",
        "y_train = train_df[TARGET]\n",
        "\n",
        "# Drop 'trip_id' from test features if it exists, as it's not a feature\n",
        "if 'trip_id' in test_df.columns:\n",
        "    X_test = test_df.drop(columns=['trip_id'])\n",
        "else:\n",
        "    X_test = test_df\n",
        "\n",
        "print(f\"X_train shape: {X_train.shape}\")\n",
        "print(f\"y_train shape: {y_train.shape}\")\n",
        "print(f\"X_test shape: {X_test.shape}\")\n",
        "\n",
        "# Data preparation for Keras model(ANN model)\n",
        "# 1. Encode the target variable (y_train) to integers (0, 1, 2...)\n",
        "# This is required for One-Hot Encoding later\n",
        "le = LabelEncoder()\n",
        "y_train_encoded = le.fit_transform(y_train)\n",
        "n_classes = len(le.classes_)\n",
        "input_dim = X_train.shape[1] # Number of input features\n",
        "\n",
        "# 2. Convert integer labels to one-hot vectors for Keras\n",
        "y_train_keras = to_categorical(y_train_encoded, num_classes=n_classes)\n",
        "\n",
        "print(f\"Keras target prepared. Input dimension: {input_dim}, Output classes: {n_classes}\")\n",
        "\n",
        "n_classes = y_train.nunique()\n",
        "print(f\"Number of target classes: {n_classes}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "7696de2f",
      "metadata": {
        "id": "7696de2f"
      },
      "source": [
        "Naive Bayes(Baseline, no tuning)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "748765b5",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "748765b5",
        "outputId": "54a6363f-adf8-48e1-f386-c746e923d48f"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Naive Bayes (GaussianNB) fitted.\n",
            "\n",
            "--- Gaussian Naive Bayes (Fixed) ---\n",
            "Accuracy: 0.5826\n",
            "Classification Report:\n",
            "               precision    recall  f1-score   support\n",
            "\n",
            "           0       0.82      0.82      0.82      6245\n",
            "           1       0.58      0.18      0.28      4911\n",
            "           2       0.27      0.90      0.42      1464\n",
            "\n",
            "    accuracy                           0.58     12620\n",
            "   macro avg       0.56      0.63      0.51     12620\n",
            "weighted avg       0.66      0.58      0.56     12620\n",
            "\n",
            "\n",
            "Sample predictions on test set:\n",
            "Naive Bayes: [2 0 0 0 0]\n",
            "submission_nb.csv saved.\n"
          ]
        }
      ],
      "source": [
        "nb_pipe = Pipeline([\n",
        "    ('imputer', SimpleImputer(strategy='median')),\n",
        "    ('scaler', StandardScaler()),\n",
        "    ('nb', GaussianNB()) # FIXED: Changed from BernoulliNB to GaussianNB\n",
        "])\n",
        "\n",
        "\n",
        "# Hyperparameter tuning grid for GaussianNB\n",
        "nb_pipe.fit(X_train, y_train)\n",
        "best_nb = nb_pipe\n",
        "print(\"Naive Bayes (GaussianNB) fitted.\")\n",
        "\n",
        "train_pred_nb = evaluate_model(best_nb, X_train, y_train, \"Gaussian Naive Bayes (Fixed)\")"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Predicting Bayes"
      ],
      "metadata": {
        "id": "lwWpmHXGd__M"
      },
      "id": "lwWpmHXGd__M"
    },
    {
      "cell_type": "code",
      "source": [
        "test_pred_nb = best_nb.predict(X_test)\n",
        "\n",
        "print(\"\\nSample predictions on test set:\")\n",
        "print(\"Naive Bayes:\", test_pred_nb[:5])\n",
        "\n",
        "# SAVE SUBMISSION FILES (WITH trip_id)\n",
        "\n",
        "# Ensure test_raw has trip_id column\n",
        "if 'trip_id' not in test_raw.columns:\n",
        "    print(\"Error: 'trip_id' column not found in test_raw/test_df! Cannot generate submission file.\")\n",
        "else:\n",
        "    submission_nb = pd.DataFrame({\n",
        "        \"trip_id\": test_raw[\"trip_id\"],\n",
        "        \"spend_category\": test_pred_nb.astype(int)\n",
        "    })\n",
        "    submission_nb.to_csv(\"submission_nb.csv\", index=False)"
      ],
      "metadata": {
        "id": "RYzRJc8seBuu"
      },
      "id": "RYzRJc8seBuu",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "id": "2e74d951",
      "metadata": {
        "id": "2e74d951"
      },
      "source": [
        "Logistic Regression"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "f9fcdd40",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "f9fcdd40",
        "outputId": "1228294c-a9a8-4dba-9efc-47b500280fd3"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Fitting 5 folds for each of 6 candidates, totalling 30 fits\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/sklearn/linear_model/_logistic.py:1247: FutureWarning: 'multi_class' was deprecated in version 1.5 and will be removed in 1.7. From then on, it will always use 'multinomial'. Leave it to its default value to avoid this warning.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Best LR Parameters: {'lr__C': 0.1, 'lr__fit_intercept': True}\n",
            "\n",
            "--- Logistic Regression (Fixed) ---\n",
            "Accuracy: 0.7478\n",
            "Classification Report:\n",
            "               precision    recall  f1-score   support\n",
            "\n",
            "           0       0.82      0.86      0.84      6245\n",
            "           1       0.67      0.73      0.70      4911\n",
            "           2       0.63      0.30      0.41      1464\n",
            "\n",
            "    accuracy                           0.75     12620\n",
            "   macro avg       0.71      0.63      0.65     12620\n",
            "weighted avg       0.74      0.75      0.74     12620\n",
            "\n",
            "\n",
            "Sample predictions on test set:\n",
            "Logistic   : [1 0 0 0 0]\n",
            "submission_lr.csv saved.\n"
          ]
        }
      ],
      "source": [
        "lr_pipe = Pipeline([\n",
        "    ('imputer', SimpleImputer(strategy='median')),\n",
        "    ('scaler', StandardScaler()),\n",
        "    ('lr', LogisticRegression(solver='lbfgs', multi_class='multinomial', random_state=42, max_iter=1000))\n",
        "# 'lbfgs' is the correct solver for multinomial loss. Increased max_iter for convergence robustness.\n",
        "])\n",
        "\n",
        "# Hyperparameter tuning grid for Logistic Regression\n",
        "lr_param_grid = {\n",
        "    'lr__C': [0.1, 1.0, 10.0],\n",
        "    'lr__fit_intercept': [True, False]\n",
        "}\n",
        "\n",
        "lr_grid_search = GridSearchCV(lr_pipe, lr_param_grid, cv=StratifiedKFold(n_splits=5), scoring='accuracy', n_jobs=-1, verbose=1)\n",
        "lr_grid_search.fit(X_train, y_train)\n",
        "best_lr = lr_grid_search.best_estimator_\n",
        "print(f\"\\nBest LR Parameters: {lr_grid_search.best_params_}\")\n",
        "\n",
        "train_pred_lr = evaluate_model(best_lr, X_train, y_train, \"Logistic Regression (Fixed)\")\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Predicting Logistic Regression"
      ],
      "metadata": {
        "id": "WbN_BeSVeJZJ"
      },
      "id": "WbN_BeSVeJZJ"
    },
    {
      "cell_type": "code",
      "source": [
        "test_pred_lr = best_lr.predict(X_test)\n",
        "\n",
        "print(\"\\nSample predictions on test set:\")\n",
        "print(\"Logistic   :\", test_pred_lr[:5])\n",
        "\n",
        "# SAVE SUBMISSION FILES (WITH trip_id)\n",
        "\n",
        "# Ensure test_raw has trip_id column\n",
        "if 'trip_id' not in test_raw.columns:\n",
        "    print(\"Error: 'trip_id' column not found in test_raw/test_df! Cannot generate submission file.\")\n",
        "else:\n",
        "    submission_lr = pd.DataFrame({\n",
        "        \"trip_id\": test_raw[\"trip_id\"],\n",
        "        \"spend_category\": test_pred_lr.astype(int)\n",
        "    })\n",
        "    submission_lr.to_csv(\"submission_lr.csv\", index=False)\n",
        "    print(\"submission_lr.csv saved.\")"
      ],
      "metadata": {
        "id": "6rrCfTVueMZJ"
      },
      "id": "6rrCfTVueMZJ",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "id": "eb1fa0d5",
      "metadata": {
        "id": "eb1fa0d5"
      },
      "source": [
        "SVM"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "112004fb",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "112004fb",
        "outputId": "b6c61fea-f8fa-472f-f116-b1c4f89d41e0"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Fitting 5 folds for each of 3 candidates, totalling 15 fits\n",
            "\n",
            "Optimized Best SVM Parameters (LinearSVC): {'lsvc__C': 1.0}\n",
            "\n",
            "--- Support Vector Machine ---\n",
            "Accuracy: 0.7387\n",
            "Classification Report:\n",
            "               precision    recall  f1-score   support\n",
            "\n",
            "           0       0.81      0.87      0.84      6245\n",
            "           1       0.66      0.74      0.70      4911\n",
            "           2       0.69      0.17      0.27      1464\n",
            "\n",
            "    accuracy                           0.74     12620\n",
            "   macro avg       0.72      0.59      0.60     12620\n",
            "weighted avg       0.74      0.74      0.72     12620\n",
            "\n",
            "\n",
            "Sample predictions on test set:\n",
            "SVM        : [1 0 0 0 0]\n",
            "submission_svm.csv saved.\n"
          ]
        }
      ],
      "source": [
        "# LinearSVC is significantly faster and more memory-efficient than SVC(kernel='linear').\n",
        "# Note: LinearSVC does not support probability=True or the 'rbf' kernel.\n",
        "lsvc_pipe = Pipeline([\n",
        "    ('imputer', SimpleImputer(strategy='median')),\n",
        "    ('scaler', StandardScaler()),\n",
        "    # dual='auto' selects the best algorithm for efficiency. max_iter increased for large data.\n",
        "    ('lsvc', LinearSVC(random_state=42, dual='auto', max_iter=10000))\n",
        "])\n",
        "\n",
        "# 1. Reduce parameter search space (only C is tunable for LinearSVC)\n",
        "lsvc_param_grid = {\n",
        "    'lsvc__C': [0.1, 1.0, 10.0],\n",
        "}\n",
        "\n",
        "# 2. Reduce n_jobs and CV folds to mitigate memory errors\n",
        "# Use n_jobs=4 (or 2) instead of -1 to limit memory consumption.\n",
        "# Use cv=3 folds for faster initial tuning.\n",
        "lsvc_grid_search = GridSearchCV(lsvc_pipe, lsvc_param_grid,\n",
        "                                cv=StratifiedKFold(n_splits=5, random_state=42, shuffle=True),\n",
        "                                scoring='accuracy',\n",
        "                                n_jobs=4, # Use a specific, limited number of cores\n",
        "                                verbose=1)\n",
        "\n",
        "lsvc_grid_search.fit(X_train, y_train)\n",
        "best_svm = lsvc_grid_search.best_estimator_\n",
        "print(f\"\\nOptimized Best SVM Parameters (LinearSVC): {lsvc_grid_search.best_params_}\")\n",
        "\n",
        "train_pred_svm = evaluate_model(best_svm, X_train, y_train, \"Support Vector Machine\")"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Predicting SVM"
      ],
      "metadata": {
        "id": "SraqsmJcePHU"
      },
      "id": "SraqsmJcePHU"
    },
    {
      "cell_type": "code",
      "source": [
        "test_pred_svm = best_svm.predict(X_test)\n",
        "\n",
        "print(\"\\nSample predictions on test set:\")\n",
        "print(\"SVM        :\", test_pred_svm[:5])\n",
        "\n",
        "# SAVE SUBMISSION FILES (WITH trip_id)\n",
        "\n",
        "# Ensure test_raw has trip_id column\n",
        "if 'trip_id' not in test_raw.columns:\n",
        "    print(\"Error: 'trip_id' column not found in test_raw/test_df! Cannot generate submission file.\")\n",
        "else:\n",
        "    submission_svm = pd.DataFrame({\n",
        "        \"trip_id\": test_raw[\"trip_id\"],\n",
        "        \"spend_category\": test_pred_svm.astype(int)\n",
        "    })\n",
        "    submission_svm.to_csv(\"submission_svm.csv\", index=False)\n",
        "    print(\"submission_svm.csv saved.\")"
      ],
      "metadata": {
        "id": "9LkGyoGPeT9S"
      },
      "id": "9LkGyoGPeT9S",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Kernelized SVM (RBF)"
      ],
      "metadata": {
        "id": "wQLtPt0jpVTw"
      },
      "id": "wQLtPt0jpVTw"
    },
    {
      "cell_type": "code",
      "source": [
        "# SVC is very sensitive to NaN and scaling\n",
        "rbf_svc_pipe = Pipeline([\n",
        "    ('imputer', SimpleImputer(strategy='median')),\n",
        "    ('scaler', StandardScaler()),\n",
        "    # Using the RBF kernel (the default)\n",
        "    ('svc', SVC(random_state=42, kernel='rbf', probability=True))\n",
        "])\n",
        "\n",
        "# NOTE: Must use a VERY limited grid to avoid excessive runtimes (only 4 fits below)\n",
        "rbf_svc_param_grid = {\n",
        "    # C controls regularization, Gamma controls the influence of single training samples\n",
        "    'svc__C': [1.0, 10.0],\n",
        "    'svc__gamma': [0.1, 'scale'] # 'scale' uses 1 / (n_features * X.var())\n",
        "}\n",
        "\n",
        "# Use only 3 cross-validation folds and limit n_jobs (if possible)\n",
        "rbf_svc_grid_search = GridSearchCV(rbf_svc_pipe, rbf_svc_param_grid,\n",
        "                                   cv=StratifiedKFold(n_splits=3, random_state=42, shuffle=True),\n",
        "                                   scoring='accuracy',\n",
        "                                   n_jobs=2, # Limit CPU cores\n",
        "                                   verbose=1)\n",
        "\n",
        "rbf_svc_grid_search.fit(X_train, y_train)\n",
        "best_rbf_svm = rbf_svc_grid_search.best_estimator_\n",
        "print(f\"\\nOptimized Best RBF SVM Parameters: {rbf_svc_grid_search.best_params_}\")\n",
        "\n",
        "train_pred_rbf_svm_eval = evaluate_model(best_rbf_svm, X_train, y_train, \"RBF SVC Classifier\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 391
        },
        "id": "xcwqcMuOpVAB",
        "outputId": "1acb9ef8-6ed2-48c4-8dff-15416023f6e9"
      },
      "id": "xcwqcMuOpVAB",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Fitting 3 folds for each of 4 candidates, totalling 12 fits\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-462650476.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     21\u001b[0m                                    verbose=1)\n\u001b[1;32m     22\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 23\u001b[0;31m \u001b[0mrbf_svc_grid_search\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     24\u001b[0m \u001b[0mbest_rbf_svm\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mrbf_svc_grid_search\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbest_estimator_\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     25\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"\\nOptimized Best RBF SVM Parameters: {rbf_svc_grid_search.best_params_}\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/sklearn/base.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(estimator, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1387\u001b[0m                 )\n\u001b[1;32m   1388\u001b[0m             ):\n\u001b[0;32m-> 1389\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0mfit_method\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mestimator\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1390\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1391\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mwrapper\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/sklearn/model_selection/_search.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, X, y, **params)\u001b[0m\n\u001b[1;32m   1022\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mresults\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1023\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1024\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_run_search\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mevaluate_candidates\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1025\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1026\u001b[0m             \u001b[0;31m# multimetric is determined here because in the case of a callable\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/sklearn/model_selection/_search.py\u001b[0m in \u001b[0;36m_run_search\u001b[0;34m(self, evaluate_candidates)\u001b[0m\n\u001b[1;32m   1569\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_run_search\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mevaluate_candidates\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1570\u001b[0m         \u001b[0;34m\"\"\"Search all candidates in param_grid\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1571\u001b[0;31m         \u001b[0mevaluate_candidates\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mParameterGrid\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparam_grid\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1572\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1573\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/sklearn/model_selection/_search.py\u001b[0m in \u001b[0;36mevaluate_candidates\u001b[0;34m(candidate_params, cv, more_results)\u001b[0m\n\u001b[1;32m    968\u001b[0m                     )\n\u001b[1;32m    969\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 970\u001b[0;31m                 out = parallel(\n\u001b[0m\u001b[1;32m    971\u001b[0m                     delayed(_fit_and_score)(\n\u001b[1;32m    972\u001b[0m                         \u001b[0mclone\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbase_estimator\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/sklearn/utils/parallel.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, iterable)\u001b[0m\n\u001b[1;32m     75\u001b[0m             \u001b[0;32mfor\u001b[0m \u001b[0mdelayed_func\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m \u001b[0;32min\u001b[0m \u001b[0miterable\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     76\u001b[0m         )\n\u001b[0;32m---> 77\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__call__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0miterable_with_config\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     78\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     79\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/joblib/parallel.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, iterable)\u001b[0m\n\u001b[1;32m   2070\u001b[0m         \u001b[0mnext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2071\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2072\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0moutput\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreturn_generator\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2073\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2074\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__repr__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/joblib/parallel.py\u001b[0m in \u001b[0;36m_get_outputs\u001b[0;34m(self, iterator, pre_dispatch)\u001b[0m\n\u001b[1;32m   1680\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1681\u001b[0m             \u001b[0;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_backend\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mretrieval_context\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1682\u001b[0;31m                 \u001b[0;32myield\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_retrieve\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1683\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1684\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mGeneratorExit\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/joblib/parallel.py\u001b[0m in \u001b[0;36m_retrieve\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1798\u001b[0m                     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jobs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_status\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0mTASK_PENDING\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1799\u001b[0m                 ):\n\u001b[0;32m-> 1800\u001b[0;31m                     \u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msleep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0.01\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1801\u001b[0m                     \u001b[0;32mcontinue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1802\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Predicting SVM"
      ],
      "metadata": {
        "id": "oYwcuxcPejAs"
      },
      "id": "oYwcuxcPejAs"
    },
    {
      "cell_type": "code",
      "source": [
        "test_pred_rbf_svm = best_rbf_svm.predict(X_test)\n",
        "\n",
        "print(\"\\nSample predictions on test set:\")\n",
        "print(\"RBF SVM    :\", test_pred_rbf_svm[:5])\n",
        "\n",
        "# SAVE SUBMISSION FILES (WITH trip_id)\n",
        "\n",
        "# Ensure test_raw has trip_id column\n",
        "if 'trip_id' not in test_raw.columns:\n",
        "    print(\"Error: 'trip_id' column not found in test_raw/test_df! Cannot generate submission file.\")\n",
        "else:\n",
        "    submission_rbf_svm = pd.DataFrame({\n",
        "    \"trip_id\": test_raw[\"trip_id\"],\n",
        "    \"spend_category\": test_pred_rbf_svm.astype(int)\n",
        "    })\n",
        "    submission_rbf_svm.to_csv(\"submission_rbf_svm.csv\", index=False)\n",
        "    print(\"submission_rbf_svm.csv saved.\")"
      ],
      "metadata": {
        "id": "N_H_ho5eelmK"
      },
      "id": "N_H_ho5eelmK",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "id": "9e0d1dde",
      "metadata": {
        "id": "9e0d1dde"
      },
      "source": [
        "ANN ( MLP Neural Network)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "7d3b952a",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7d3b952a",
        "outputId": "c5b6662f-9156-4733-d0ca-3c0873bc7565"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/keras/src/layers/core/dense.py:93: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
            "  super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training ANN (MLP) Model Directly...\n",
            "ANN (MLP) Training Complete.\n",
            "\n",
            "Sample predictions on test set:\n",
            "ANN (MLP)  : [1 0 0 1 0]\n",
            "submission_mlp.csv saved.\n"
          ]
        }
      ],
      "source": [
        "# NOTE: The build_mlp_model function must be defined in the notebook prior to this cell.\n",
        "\n",
        "# 1. Define the parameters for the best model (using your intended values)\n",
        "BEST_MLP_PARAMS = {\n",
        "    'units': 64,             # Neurons per layer\n",
        "    'layers': 2,             # Number of hidden layers\n",
        "    'learning_rate': 0.001,\n",
        "}\n",
        "\n",
        "def build_mlp_model(input_dim, output_dim, layers=2, units=64, learning_rate=0.001):\n",
        "    \"\"\"Creates a basic Multilayer Perceptron (ANN) model.\"\"\"\n",
        "\n",
        "    # Ensure imports are available: from keras.models import Sequential\n",
        "    # from keras.layers import Dense, Dropout\n",
        "    # import tensorflow as tf # for keras.optimizers.Adam\n",
        "\n",
        "    model = Sequential()\n",
        "\n",
        "    # Input Layer and First Hidden Layer\n",
        "    # Note: input_dim should be explicitly defined for the first layer\n",
        "    model.add(Dense(units=units, activation='relu', input_dim=input_dim))\n",
        "    model.add(Dropout(0.2)) # Standard regularization to prevent overfitting\n",
        "\n",
        "    # Additional Hidden Layers\n",
        "    for _ in range(layers - 1):\n",
        "        model.add(Dense(units=units, activation='relu'))\n",
        "        model.add(Dropout(0.2))\n",
        "\n",
        "    # Output Layer (Softmax for multi-class classification)\n",
        "    model.add(Dense(units=output_dim, activation='softmax'))\n",
        "\n",
        "    # Compile model\n",
        "    optimizer = tf.keras.optimizers.Adam(learning_rate=learning_rate)\n",
        "    model.compile(optimizer=optimizer, loss='categorical_crossentropy', metrics=['accuracy'])\n",
        "\n",
        "    return model\n",
        "\n",
        "# 2. Build the Model\n",
        "# Assuming input_dim and n_classes are defined in an earlier cell\n",
        "best_mlp_model = build_mlp_model(\n",
        "    input_dim=X_train.shape[1],\n",
        "    output_dim=n_classes,\n",
        "    **BEST_MLP_PARAMS\n",
        ")\n",
        "\n",
        "print(\"Training ANN (MLP) Model Directly...\")\n",
        "\n",
        "# 3. Train the Model Directly (Bypassing GridSearchCV)\n",
        "# We use a validation split to monitor performance during training\n",
        "history = best_mlp_model.fit(\n",
        "    X_train,\n",
        "    y_train_keras,          # Use the ONE-HOT ENCODED target\n",
        "    epochs=50,\n",
        "    batch_size=64,          # Using a batch size from your original grid\n",
        "    validation_split=0.1,\n",
        "    verbose=0 # Set to 1 if you want to see the progress\n",
        ")\n",
        "\n",
        "# --- 4. Generate Predictions and Decode ---\n",
        "\n",
        "# Generate probabilities for the test set\n",
        "y_pred_proba_mlp = best_mlp_model.predict(X_test, verbose=0)\n",
        "\n",
        "# Convert probabilities back to integer class indices (0, 1, 2...)\n",
        "y_pred_encoded = np.argmax(y_pred_proba_mlp, axis=1)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Predicting Neural Network"
      ],
      "metadata": {
        "id": "kCD2i0G4e7Vb"
      },
      "id": "kCD2i0G4e7Vb"
    },
    {
      "cell_type": "code",
      "source": [
        "# Decode predictions back to original spend categories (1.0, 2.0, 3.0...)\n",
        "# Assume 'le' (LabelEncoder) is available from the data prep step\n",
        "test_pred_mlp = le.inverse_transform(y_pred_encoded)\n",
        "\n",
        "# Generate train predictions for evaluation (optional, but requested)\n",
        "y_train_proba_mlp = best_mlp_model.predict(X_train, verbose=0)\n",
        "y_train_pred_encoded = np.argmax(y_train_proba_mlp, axis=1)\n",
        "train_pred_mlp = le.inverse_transform(y_train_pred_encoded)\n",
        "\n",
        "\n",
        "print(\"ANN (MLP) Training Complete.\")\n",
        "print(\"\\nSample predictions on test set:\")\n",
        "print(\"ANN (MLP)  :\", test_pred_mlp[:5])\n",
        "\n",
        "# --- 5. SAVE SUBMISSION FILES (WITH trip_id) ---\n",
        "\n",
        "# Ensure test_raw has trip_id column\n",
        "if 'trip_id' not in test_raw.columns:\n",
        "    print(\"Error: 'trip_id' column not found in test_raw/test_df! Cannot generate submission file.\")\n",
        "else:\n",
        "    submission_mlp = pd.DataFrame({\n",
        "        \"trip_id\": test_raw[\"trip_id\"],\n",
        "        \"spend_category\": test_pred_mlp.astype(int)\n",
        "    })\n",
        "    submission_mlp.to_csv(\"submission_mlp.csv\", index=False)"
      ],
      "metadata": {
        "id": "7kHEc1xae_CF"
      },
      "id": "7kHEc1xae_CF",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "id": "2110e4cc",
      "metadata": {
        "id": "2110e4cc"
      },
      "source": [
        "K-Means Clustering"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "271739a6",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "271739a6",
        "outputId": "9cd0cee2-d829-4e8a-98e7-52cca6d3b90c"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "K-Means clustering fitted successfully.\n"
          ]
        }
      ],
      "source": [
        "# Used for finding natural groups in the features (X_train).\n",
        "# Wrap K-Means in a pipeline to ensure data is clean and scaled\n",
        "kmeans_pipe = Pipeline([\n",
        "    ('imputer', SimpleImputer(strategy='median')), # Catches remaining NaNs\n",
        "    ('scaler', StandardScaler()),                 # Scales features for distance calculation\n",
        "    # Used for finding natural groups in the features (X_train).\n",
        "    ('kmeans', KMeans(n_clusters=n_classes,\n",
        "                      random_state=42,\n",
        "                      n_init='auto'))\n",
        "])\n",
        "kmeans_pipe.fit(X_train)\n",
        "kmeans_model = kmeans_pipe.named_steps['kmeans'] # Extract the fitted model for analysis\n",
        "print(\"K-Means clustering fitted successfully.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "e64864b3",
      "metadata": {
        "id": "e64864b3"
      },
      "source": [
        "Gaussian Mixture Model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "0713d80e",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0713d80e",
        "outputId": "0775f73b-9715-4943-ff1e-768e8f98b24d"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "GMM fitted successfully.\n"
          ]
        }
      ],
      "source": [
        "# Wrap GMM in a pipeline to ensure data is clean and scaled\n",
        "gmm_pipe = Pipeline([\n",
        "    ('imputer', SimpleImputer(strategy='median')), # Catches remaining NaNs\n",
        "    ('scaler', StandardScaler()),                 # Scales features for proper covariance estimation\n",
        "    ('gmm', GMM(n_components=n_classes,\n",
        "                           random_state=42))\n",
        "])\n",
        "\n",
        "# Fit the pipeline to X_train.\n",
        "gmm_pipe.fit(X_train)\n",
        "gmm_model = gmm_pipe.named_steps['gmm'] # Extract the fitted model for analysis\n",
        "print(\"GMM fitted successfully.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "26edac83",
      "metadata": {
        "id": "26edac83"
      },
      "source": [
        "XGBoost"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "9ff74d24",
      "metadata": {
        "id": "9ff74d24"
      },
      "outputs": [],
      "source": [
        "xgb_pipe = Pipeline([\n",
        "    ('imputer', SimpleImputer(strategy='median')),\n",
        "    # XGBoost does not need a scaler\n",
        "    ('xgb', XGBClassifier(objective='multi:softmax',\n",
        "                          num_class=n_classes,\n",
        "                          use_label_encoder=False, # Suppress warning on older versions\n",
        "                          eval_metric='mlogloss', # Standard multi-class metric\n",
        "                          random_state=42))\n",
        "])\n",
        "\n",
        "# Using a reduced grid for faster initial results (cv=3, n_jobs=4)\n",
        "xgb_param_grid = {\n",
        "    'xgb__n_estimators': [100, 200],\n",
        "    'xgb__learning_rate': [0.05, 0.1],\n",
        "    'xgb__max_depth': [3, 5]\n",
        "}\n",
        "\n",
        "xgb_grid_search = GridSearchCV(xgb_pipe, xgb_param_grid, cv=StratifiedKFold(n_splits=3), scoring='accuracy', n_jobs=4, verbose=1)\n",
        "xgb_grid_search.fit(X_train, y_train)\n",
        "best_xgb = xgb_grid_search.best_estimator_\n",
        "print(f\"\\nBest XGBoost Parameters: {xgb_grid_search.best_params_}\")\n",
        "\n",
        "train_pred_xgb = evaluate_model(best_xgb, X_train, y_train, \"XGBoost Classifier\")\n",
        "test_pred_xgb = best_xgb.predict(X_test)\n",
        "\n",
        "print(\"\\nSample predictions on test set:\")\n",
        "print(\"XGBoost    :\", test_pred_xgb[:5])\n",
        "\n",
        "# SAVE SUBMISSION FILES (WITH trip_id)\n",
        "\n",
        "# Ensure test_raw has trip_id column\n",
        "if 'trip_id' not in test_raw.columns:\n",
        "    print(\"Error: 'trip_id' column not found in test_raw/test_df! Cannot generate submission file.\")\n",
        "else:\n",
        "    submission_xgb = pd.DataFrame({\n",
        "        \"trip_id\": test_raw[\"trip_id\"],\n",
        "        \"spend_category\": test_pred_xgb.astype(int)\n",
        "    })\n",
        "    submission_xgb.to_csv(\"submission_xgb.csv\", index=False)\n",
        "    print(\"submission_xgb.csv saved.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "8f4dd599",
      "metadata": {
        "id": "8f4dd599"
      },
      "source": [
        "LightGBM"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "4e988564",
      "metadata": {
        "id": "4e988564"
      },
      "outputs": [],
      "source": [
        "lgbm_pipe = Pipeline([\n",
        "    # LightGBM does not need a scaler\n",
        "    ('lgbm', LGBMClassifier(objective='multiclass',\n",
        "                            num_class=n_classes,\n",
        "                            metric='multi_logloss',\n",
        "                            random_state=42,\n",
        "                            n_jobs=-1))\n",
        "])\n",
        "\n",
        "# Using a reduced grid for faster initial results (cv=3, n_jobs=4)\n",
        "lgbm_param_grid = {\n",
        "    'lgbm__n_estimators': [100, 200],\n",
        "    'lgbm__learning_rate': [0.05, 0.1],\n",
        "    'lgbm__num_leaves': [20, 31]\n",
        "}\n",
        "\n",
        "lgbm_grid_search = GridSearchCV(lgbm_pipe, lgbm_param_grid, cv=StratifiedKFold(n_splits=3), scoring='accuracy', n_jobs=4, verbose=1)\n",
        "lgbm_grid_search.fit(X_train, y_train)\n",
        "best_lgbm = lgbm_grid_search.best_estimator_\n",
        "print(f\"\\nBest LightGBM Parameters: {lgbm_grid_search.best_params_}\")\n",
        "\n",
        "train_pred_lgbm = evaluate_model(best_lgbm, X_train, y_train, \"LightGBM Classifier\")\n",
        "test_pred_lgbm = best_lgbm.predict(X_test)\n",
        "\n",
        "print(\"\\nSample predictions on test set:\")\n",
        "print(\"LightGBM   :\", test_pred_lgbm[:5])\n",
        "\n",
        "# SAVE SUBMISSION FILES (WITH trip_id)\n",
        "\n",
        "# Ensure test_raw has trip_id column\n",
        "if 'trip_id' not in test_raw.columns:\n",
        "    print(\"Error: 'trip_id' column not found in test_raw/test_df! Cannot generate submission file.\")\n",
        "else:\n",
        "    submission_lgbm = pd.DataFrame({\n",
        "        \"trip_id\": test_raw[\"trip_id\"],\n",
        "        \"spend_category\": test_pred_lgbm.astype(int)\n",
        "    })\n",
        "    submission_lgbm.to_csv(\"submission_lgbm.csv\", index=False)\n",
        "    print(\"submission_lgbm.csv saved.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "a41bb675",
      "metadata": {
        "id": "a41bb675"
      },
      "source": [
        "CatBoost"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "9ef7fbab",
      "metadata": {
        "id": "9ef7fbab"
      },
      "outputs": [],
      "source": [
        "# --- CATBOOST-SPECIFIC DATA PREPARATION (Using Raw Files) ---\n",
        "import re\n",
        "\n",
        "# 1. Re-define helper functions used in original preprocessing (crucial for consistency)\n",
        "def map_range_to_midpoint(range_str):\n",
        "    try:\n",
        "        if '-' in range_str:\n",
        "            start, end = map(float, re.findall(r'\\d+', range_str))\n",
        "            return (start + end) / 2\n",
        "        else: # Handle single number values\n",
        "            return float(range_str)\n",
        "    except:\n",
        "        return np.nan # Use NaN for CatBoost to impute\n",
        "\n",
        "age_mapping = {'<18': 10, '18-24': 21, '25-44': 35, '45-64': 55, '65+': 70}\n",
        "\n",
        "# 2. Load Raw Data\n",
        "raw_train_df = pd.read_csv(\"train.csv\")\n",
        "raw_test_df = pd.read_csv(\"test.csv\")\n",
        "TARGET = 'spend_category'\n",
        "\n",
        "# 3. Clean Target and Store Test ID\n",
        "raw_train_df.dropna(subset=[TARGET], inplace=True)\n",
        "y_train_cbt = raw_train_df[TARGET].astype(int)\n",
        "\n",
        "# 4. Apply Feature Engineering (Ordinal Mappings)\n",
        "for df in [raw_train_df, raw_test_df]:\n",
        "    # Apply Midpoint mapping\n",
        "    df['days_booked_midpoint'] = df['days_booked_before_trip'].apply(map_range_to_midpoint)\n",
        "    df['total_trip_days_midpoint'] = df['total_trip_days'].apply(map_range_to_midpoint)\n",
        "    # Apply Age Ordinal mapping\n",
        "    df['age_group_ordinal'] = df['age_group'].map(age_mapping).fillna(np.nan)\n",
        "\n",
        "    # Drop original ordinal columns\n",
        "    df.drop(columns=['days_booked_before_trip', 'total_trip_days', 'age_group'], inplace=True)\n",
        "\n",
        "# 5. Separate X and Identify Categorical Features\n",
        "X_train_cbt = raw_train_df.drop(columns=[TARGET, 'trip_id'])\n",
        "X_test_cbt = raw_test_df.drop(columns=['trip_id'])\n",
        "\n",
        "# CatBoost needs the list of *string* column names that are categorical\n",
        "CBT_CAT_FEATURES = X_train_cbt.select_dtypes(include=['object']).columns.tolist()\n",
        "\n",
        "print(f\"\\nCatBoost Data Ready.\")\n",
        "print(f\"Number of string categorical features identified: {len(CBT_CAT_FEATURES)}\")\n",
        "# --------------------------------------------------------------------------------\n",
        "\n",
        "\n",
        "cbt_pipe = Pipeline([\n",
        "    ('cbt', CatBoostClassifier(verbose=0,\n",
        "                                random_state=42,\n",
        "                                loss_function='MultiClass',\n",
        "                                eval_metric='MultiClass',\n",
        "                                # Pass the categorical feature names to CatBoost\n",
        "                                cat_features=CBT_CAT_FEATURES, # <-- THIS IS THE KEY CHANGE\n",
        "                                thread_count=4))\n",
        "])\n",
        "\n",
        "\n",
        "\n",
        "#Hyperparameter tuning grid for CatBoost\n",
        "cbt_param_grid = {\n",
        "    'cbt__n_estimators': [100, 200],\n",
        "    'cbt__learning_rate': [0.05, 0.1]\n",
        "}\n",
        "\n",
        "cbt_grid_search = GridSearchCV(cbt_pipe, cbt_param_grid, cv=StratifiedKFold(n_splits=3), scoring='accuracy', n_jobs=4, verbose=1)\n",
        "\n",
        "# Fit CatBoost with the specially prepared raw data\n",
        "cbt_grid_search.fit(X_train_cbt, y_train_cbt) # <-- USE CBT VARIABLES\n",
        "best_cbt = cbt_grid_search.best_estimator_\n",
        "print(f\"\\nBest CatBoost Parameters: {cbt_grid_search.best_params_}\")\n",
        "\n",
        "train_pred_cbt = evaluate_model(best_cbt, X_train_cbt, y_train_cbt, \"CatBoost Classifier\")\n",
        "test_pred_cbt = best_cbt.predict(X_test_cbt)\n",
        "\n",
        "print(\"\\nSample predictions on test set:\")\n",
        "# NOTE: CatBoost multi-class prediction returns a 2D array, so we flatten it\n",
        "test_pred_cbt_flat = test_pred_cbt.flatten()\n",
        "\n",
        "print(\"CatBoost   :\", test_pred_cbt_flat[:5])\n",
        "\n",
        "# SAVE SUBMISSION FILES (WITH trip_id)\n",
        "\n",
        "# Ensure test_raw has trip_id column\n",
        "if 'trip_id' not in test_raw.columns:\n",
        "    print(\"Error: 'trip_id' column not found in test_raw/test_df! Cannot generate submission file.\")\n",
        "else:\n",
        "    submission_cbt = pd.DataFrame({\n",
        "    # Use the raw_test_id created during CatBoost prep\n",
        "    \"trip_id\": test_raw[\"trip_id\"],\n",
        "    \"spend_category\": test_pred_cbt_flat.astype(int)\n",
        "    })\n",
        "    submission_cbt.to_csv(\"submission_cbt.csv\", index=False)\n",
        "    print(\"submission_cbt.csv saved.\")"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "mlpro_env (3.11.4)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.4"
    },
    "colab": {
      "provenance": []
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}