{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a689ddb",
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'lightgbm'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mModuleNotFoundError\u001b[39m                       Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[1]\u001b[39m\u001b[32m, line 21\u001b[39m\n\u001b[32m     19\u001b[39m \u001b[38;5;66;03m# Ensemble Models\u001b[39;00m\n\u001b[32m     20\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mxgboost\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m XGBClassifier\n\u001b[32m---> \u001b[39m\u001b[32m21\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mlightgbm\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m LGBMClassifier\n\u001b[32m     23\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mmatplotlib\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mpyplot\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mplt\u001b[39;00m\n",
      "\u001b[31mModuleNotFoundError\u001b[39m: No module named 'lightgbm'"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "from sklearn.model_selection import train_test_split, StratifiedKFold, GridSearchCV\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import accuracy_score, classification_report\n",
    "from sklearn.metrics import ConfusionMatrixDisplay\n",
    "\n",
    "from sklearn.pipeline import Pipeline\n",
    "\n",
    "from sklearn.naive_bayes import GaussianNB \n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.svm import LinearSVC\n",
    "\n",
    "# New Imports required for the new models\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.mixture import GaussianMixture as GMM\n",
    "\n",
    "# Ensemble Models\n",
    "from xgboost import XGBClassifier\n",
    "from lightgbm import LGBMClassifier\n",
    "from catboost import CatBoostClassifier\n",
    "\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "41c44e53",
   "metadata": {},
   "source": [
    "Load processed data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae7efd6b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Processed data (one-hot / True-False etc.)\n",
    "train_df = pd.read_csv(\"train_processed.csv\")\n",
    "test_df = pd.read_csv(\"test_processed.csv\")\n",
    "\n",
    "# Assuming 'test_raw.csv' was used to get the original trip_id column\n",
    "# If not available, replace 'test_raw.csv' with the actual filename containing 'trip_id'\n",
    "try:\n",
    "    test_raw = pd.read_csv('test_raw.csv')\n",
    "except FileNotFoundError:\n",
    "    print(\"Warning: 'test_raw.csv' not found. Assuming 'test_processed.csv' contains 'trip_id'.\")\n",
    "    test_raw = test_df.copy()\n",
    "\n",
    "print(f\"Train Data Shape: {train_df.shape}\")\n",
    "print(f\"Test Data Shape: {test_df.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aacb8afc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- FIX: Clean column names for LightGBM compatibility ---\n",
    "import re\n",
    "\n",
    "def clean_feature_names(df):\n",
    "    \"\"\"\n",
    "    Cleans column names by replacing problematic characters (commas, brackets,\n",
    "    and other special symbols) with underscores for LightGBM compatibility.\n",
    "    \"\"\"\n",
    "    new_cols = []\n",
    "    for col in df.columns:\n",
    "        # 1. Replace commas and brackets with underscores\n",
    "        cleaned_col = col.replace(',', '_').replace('[', '_').replace(']', '_')\n",
    "        # 2. Replace any remaining non-alphanumeric/non-underscore sequence with a single underscore\n",
    "        cleaned_col = re.sub(r'[^A-Za-z0-9_]+', '_', cleaned_col)\n",
    "        # 3. Clean up leading/trailing underscores and double underscores\n",
    "        cleaned_col = cleaned_col.strip('_').replace('__', '_')\n",
    "        new_cols.append(cleaned_col)\n",
    "    \n",
    "    df.columns = new_cols\n",
    "    return df\n",
    "\n",
    "# Apply the cleaning to both dataframes before splitting\n",
    "train_df = clean_feature_names(train_df)\n",
    "test_df = clean_feature_names(test_df)\n",
    "\n",
    "print(\"Feature names cleaned for LightGBM compatibility.\")\n",
    "# -----------------------------------------------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "36d0cb00",
   "metadata": {},
   "source": [
    "Cross-Validation Split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ecd27abd",
   "metadata": {},
   "outputs": [],
   "source": [
    "TARGET = 'spend_category'\n",
    "\n",
    "X_train = train_df.drop(columns=[TARGET])\n",
    "y_train = train_df[TARGET]\n",
    "\n",
    "# Drop 'trip_id' from test features if it exists, as it's not a feature\n",
    "if 'trip_id' in test_df.columns:\n",
    "    X_test = test_df.drop(columns=['trip_id'])\n",
    "else:\n",
    "    X_test = test_df\n",
    "\n",
    "print(f\"X_train shape: {X_train.shape}\")\n",
    "print(f\"y_train shape: {y_train.shape}\")\n",
    "print(f\"X_test shape: {X_test.shape}\")\n",
    "\n",
    "n_classes = y_train.nunique()\n",
    "print(f\"Number of target classes: {n_classes}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7696de2f",
   "metadata": {},
   "source": [
    "Naive Bayes(Baseline, no tuning)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "748765b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "nb_pipe = Pipeline([\n",
    "    ('scaler', StandardScaler()),\n",
    "    ('nb', GaussianNB()) # FIXED: Changed from BernoulliNB to GaussianNB\n",
    "])\n",
    "\n",
    "\n",
    "# Hyperparameter tuning grid for GaussianNB\n",
    "nb_pipe.fit(X_train, y_train)\n",
    "best_nb = nb_pipe\n",
    "print(\"Naive Bayes (GaussianNB) fitted.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e74d951",
   "metadata": {},
   "source": [
    "Logistic Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9fcdd40",
   "metadata": {},
   "outputs": [],
   "source": [
    "lr_pipe = Pipeline([\n",
    "    ('scaler', StandardScaler()),\n",
    "    ('lr', LogisticRegression(solver='lbfgs', multi_class='multinomial', random_state=42, max_iter=1000))\n",
    "# 'lbfgs' is the correct solver for multinomial loss. Increased max_iter for convergence robustness.\n",
    "])\n",
    "\n",
    "# Hyperparameter tuning grid for Logistic Regression\n",
    "lr_param_grid = {\n",
    "    'lr__C': [0.1, 1.0, 10.0]\n",
    "}\n",
    "\n",
    "lr_grid_search = GridSearchCV(lr_pipe, lr_param_grid, cv=StratifiedKFold(n_splits=5), scoring='accuracy', n_jobs=-1, verbose=1)\n",
    "lr_grid_search.fit(X_train, y_train)\n",
    "best_lr = lr_grid_search.best_estimator_\n",
    "print(f\"\\nBest LR Parameters: {lr_grid_search.best_params_}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb1fa0d5",
   "metadata": {},
   "source": [
    "SVM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "112004fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# LinearSVC is significantly faster and more memory-efficient than SVC(kernel='linear').\n",
    "# Note: LinearSVC does not support probability=True or the 'rbf' kernel.\n",
    "lsvc_pipe = Pipeline([\n",
    "    ('scaler', StandardScaler()),\n",
    "    # dual='auto' selects the best algorithm for efficiency. max_iter increased for large data.\n",
    "    ('lsvc', LinearSVC(random_state=42, dual='auto', max_iter=10000)) \n",
    "])\n",
    "\n",
    "# 1. Reduce parameter search space (only C is tunable for LinearSVC)\n",
    "lsvc_param_grid = {\n",
    "    'lsvc__C': [0.1, 1.0, 10.0]\n",
    "}\n",
    "\n",
    "# 2. Reduce n_jobs and CV folds to mitigate memory errors\n",
    "# Use n_jobs=4 (or 2) instead of -1 to limit memory consumption.\n",
    "# Use cv=3 folds for faster initial tuning.\n",
    "lsvc_grid_search = GridSearchCV(lsvc_pipe, lsvc_param_grid, \n",
    "                                cv=StratifiedKFold(n_splits=5, random_state=42, shuffle=True), \n",
    "                                scoring='accuracy', \n",
    "                                n_jobs=4, # Use a specific, limited number of cores\n",
    "                                verbose=1)\n",
    "\n",
    "lsvc_grid_search.fit(X_train, y_train)\n",
    "best_svm = lsvc_grid_search.best_estimator_\n",
    "print(f\"\\nOptimized Best SVM Parameters (LinearSVC): {lsvc_grid_search.best_params_}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2110e4cc",
   "metadata": {},
   "source": [
    "K-Means Clustering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "271739a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Used for finding natural groups in the features (X_train).\n",
    "# Set n_clusters to match the number of target classes for direct comparison.\n",
    "kmeans_model = KMeans(n_clusters=n_classes, random_state=42, n_init='auto')\n",
    "\n",
    "# K-Means will run fast\n",
    "kmeans_model.fit(X_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e64864b3",
   "metadata": {},
   "source": [
    "Gaussian Mixture Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0713d80e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# A probabilistic approach to clustering, assuming data is generated from a mixture of Gaussian distributions.\n",
    "gmm_model = GMM(n_components=n_classes, random_state=42)\n",
    "\n",
    "#GMM is more computationally intensive\n",
    "gmm_model.fit(X_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26edac83",
   "metadata": {},
   "source": [
    "XGBoost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ff74d24",
   "metadata": {},
   "outputs": [],
   "source": [
    "xgb_pipe = Pipeline([\n",
    "    # XGBoost does not need a scaler\n",
    "    ('xgb', XGBClassifier(objective='multi:softmax',\n",
    "                          num_class=n_classes,\n",
    "                          use_label_encoder=False, # Suppress warning on older versions\n",
    "                          eval_metric='mlogloss', # Standard multi-class metric\n",
    "                          random_state=42))\n",
    "])\n",
    "\n",
    "# Using a reduced grid for faster initial results (cv=3, n_jobs=4)\n",
    "xgb_param_grid = {\n",
    "    'xgb__n_estimators': [100, 200],\n",
    "    'xgb__learning_rate': [0.05, 0.1],\n",
    "    'xgb__max_depth': [3, 5]\n",
    "}\n",
    "\n",
    "xgb_grid_search = GridSearchCV(xgb_pipe, xgb_param_grid, cv=StratifiedKFold(n_splits=3), scoring='accuracy', n_jobs=4, verbose=1)\n",
    "xgb_grid_search.fit(X_train, y_train)\n",
    "best_xgb = xgb_grid_search.best_estimator_\n",
    "print(f\"\\nBest XGBoost Parameters: {xgb_grid_search.best_params_}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f4dd599",
   "metadata": {},
   "source": [
    "LightGBM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e988564",
   "metadata": {},
   "outputs": [],
   "source": [
    "lgbm_pipe = Pipeline([\n",
    "    # LightGBM does not need a scaler\n",
    "    ('lgbm', LGBMClassifier(objective='multiclass',\n",
    "                            num_class=n_classes,\n",
    "                            metric='multi_logloss',\n",
    "                            random_state=42,\n",
    "                            n_jobs=-1))\n",
    "])\n",
    "\n",
    "# Using a reduced grid for faster initial results (cv=3, n_jobs=4)\n",
    "lgbm_param_grid = {\n",
    "    'lgbm__n_estimators': [100, 200],\n",
    "    'lgbm__learning_rate': [0.05, 0.1],\n",
    "    'lgbm__num_leaves': [20, 31]\n",
    "}\n",
    "\n",
    "lgbm_grid_search = GridSearchCV(lgbm_pipe, lgbm_param_grid, cv=StratifiedKFold(n_splits=3), scoring='accuracy', n_jobs=4, verbose=1)\n",
    "lgbm_grid_search.fit(X_train, y_train)\n",
    "best_lgbm = lgbm_grid_search.best_estimator_\n",
    "print(f\"\\nBest LightGBM Parameters: {lgbm_grid_search.best_params_}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a41bb675",
   "metadata": {},
   "source": [
    "CatBoost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ef7fbab",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- CATBOOST-SPECIFIC DATA PREPARATION (Using Raw Files) ---\n",
    "import re\n",
    "\n",
    "# 1. Re-define helper functions used in original preprocessing (crucial for consistency)\n",
    "def map_range_to_midpoint(range_str):\n",
    "    try:\n",
    "        if '-' in range_str:\n",
    "            start, end = map(float, re.findall(r'\\d+', range_str))\n",
    "            return (start + end) / 2\n",
    "        else: # Handle single number values\n",
    "            return float(range_str)\n",
    "    except:\n",
    "        return np.nan # Use NaN for CatBoost to impute\n",
    "\n",
    "age_mapping = {'<18': 10, '18-24': 21, '25-44': 35, '45-64': 55, '65+': 70}\n",
    "\n",
    "# 2. Load Raw Data\n",
    "raw_train_df = pd.read_csv(\"train.csv\")\n",
    "raw_test_df = pd.read_csv(\"test.csv\")\n",
    "TARGET = 'spend_category'\n",
    "\n",
    "# 3. Clean Target and Store Test ID\n",
    "raw_train_df.dropna(subset=[TARGET], inplace=True)\n",
    "y_train_cbt = raw_train_df[TARGET].astype(int)\n",
    "\n",
    "# 4. Apply Feature Engineering (Ordinal Mappings)\n",
    "for df in [raw_train_df, raw_test_df]:\n",
    "    # Apply Midpoint mapping\n",
    "    df['days_booked_midpoint'] = df['days_booked_before_trip'].apply(map_range_to_midpoint)\n",
    "    df['total_trip_days_midpoint'] = df['total_trip_days'].apply(map_range_to_midpoint)\n",
    "    # Apply Age Ordinal mapping\n",
    "    df['age_group_ordinal'] = df['age_group'].map(age_mapping).fillna(np.nan)\n",
    "\n",
    "    # Drop original ordinal columns\n",
    "    df.drop(columns=['days_booked_before_trip', 'total_trip_days', 'age_group'], inplace=True)\n",
    "\n",
    "# 5. Separate X and Identify Categorical Features\n",
    "X_train_cbt = raw_train_df.drop(columns=[TARGET, 'trip_id'])\n",
    "X_test_cbt = raw_test_df.drop(columns=['trip_id'])\n",
    "\n",
    "# CatBoost needs the list of *string* column names that are categorical\n",
    "CBT_CAT_FEATURES = X_train_cbt.select_dtypes(include=['object']).columns.tolist()\n",
    "\n",
    "print(f\"\\nCatBoost Data Ready.\")\n",
    "print(f\"Number of string categorical features identified: {len(CBT_CAT_FEATURES)}\")\n",
    "# --------------------------------------------------------------------------------\n",
    "\n",
    "\n",
    "cbt_pipe = Pipeline([\n",
    "    ('cbt', CatBoostClassifier(verbose=0,\n",
    "                                random_state=42, \n",
    "                                loss_function='MultiClass', \n",
    "                                eval_metric='MultiClass',\n",
    "                                # Pass the categorical feature names to CatBoost\n",
    "                                cat_features=CBT_CAT_FEATURES, # <-- THIS IS THE KEY CHANGE\n",
    "                                thread_count=4)) \n",
    "])\n",
    "\n",
    "\n",
    "\n",
    "#Hyperparameter tuning grid for CatBoost\n",
    "cbt_param_grid = {\n",
    "    'cbt__n_estimators': [100, 200],\n",
    "    'cbt__learning_rate': [0.05, 0.1]\n",
    "}\n",
    "\n",
    "cbt_grid_search = GridSearchCV(cbt_pipe, cbt_param_grid, cv=StratifiedKFold(n_splits=3), scoring='accuracy', n_jobs=4, verbose=1)\n",
    "\n",
    "# Fit CatBoost with the specially prepared raw data\n",
    "cbt_grid_search.fit(X_train_cbt, y_train_cbt) # <-- USE CBT VARIABLES\n",
    "best_cbt = cbt_grid_search.best_estimator_\n",
    "print(f\"\\nBest CatBoost Parameters: {cbt_grid_search.best_params_}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c84c982",
   "metadata": {},
   "source": [
    "Evaluate models on Training data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1c03c94",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_model(model, X, y, model_name):\n",
    "    y_pred = model.predict(X)\n",
    "    accuracy = accuracy_score(y, y_pred)\n",
    "    print(f\"\\n--- {model_name} ---\")\n",
    "    print(f\"Accuracy: {accuracy:.4f}\")\n",
    "    print(\"Classification Report:\\n\", classification_report(y, y_pred, zero_division=0))\n",
    "\n",
    "    # Optional: Display confusion matrix\n",
    "    # disp = ConfusionMatrixDisplay.from_estimator(model, X, y, normalize='true')\n",
    "    # disp.ax_.set_title(f\"Confusion Matrix for {model_name}\")\n",
    "    # plt.show()\n",
    "\n",
    "    return y_pred\n",
    "\n",
    "train_pred_nb = evaluate_model(best_nb, X_train, y_train, \"Gaussian Naive Bayes (Fixed)\")\n",
    "train_pred_lr = evaluate_model(best_lr, X_train, y_train, \"Logistic Regression (Fixed)\")\n",
    "train_pred_svm = evaluate_model(best_svm, X_train, y_train, \"Support Vector Machine\")\n",
    "train_pred_xgb = evaluate_model(best_xgb, X_train, y_train, \"XGBoost Classifier\")\n",
    "train_pred_lgbm = evaluate_model(best_lgbm, X_train, y_train, \"LightGBM Classifier\")\n",
    "train_pred_cbt = evaluate_model(best_cbt, X_train_cbt, y_train_cbt, \"CatBoost Classifier\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "75e7f7c1",
   "metadata": {},
   "source": [
    "Make predictions on Test Data and Save Submission"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0f33eac",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_pred_nb = best_nb.predict(X_test)\n",
    "test_pred_lr = best_lr.predict(X_test)\n",
    "test_pred_svm = best_svm.predict(X_test)\n",
    "test_pred_xgb = best_xgb.predict(X_test)\n",
    "test_pred_lgbm = best_lgbm.predict(X_test)\n",
    "test_pred_cbt = best_cbt.predict(X_test_cbt) # <-- New\n",
    "\n",
    "print(\"\\nPredictions generated for the test set.\")\n",
    "\n",
    "print(\"\\nSample predictions on test set:\")\n",
    "print(\"Naive Bayes:\", test_pred_nb[:5])\n",
    "print(\"Logistic   :\", test_pred_lr[:5])\n",
    "print(\"SVM        :\", test_pred_svm[:5])\n",
    "print(\"XGBoost    :\", test_pred_xgb[:5])\n",
    "print(\"LightGBM   :\", test_pred_lgbm[:5]) \n",
    "# NOTE: CatBoost multi-class prediction returns a 2D array, so we flatten it\n",
    "test_pred_cbt_flat = test_pred_cbt.flatten()\n",
    "\n",
    "print(\"CatBoost   :\", test_pred_cbt_flat[:5]) # <-- New\n",
    "\n",
    "\n",
    "# SAVE SUBMISSION FILES (WITH trip_id)\n",
    "\n",
    "# Ensure test_raw has trip_id column\n",
    "if 'trip_id' not in test_raw.columns:\n",
    "    print(\"Error: 'trip_id' column not found in test_raw/test_df! Cannot generate submission file.\")\n",
    "else:\n",
    "    submission_nb = pd.DataFrame({\n",
    "        \"trip_id\": test_raw[\"trip_id\"],\n",
    "        \"spend_category\": test_pred_nb.astype(int)\n",
    "    })\n",
    "    submission_nb.to_csv(\"submission_nb.csv\", index=False)\n",
    "    print(\"submission_nb.csv saved.\")\n",
    "\n",
    "    submission_lr = pd.DataFrame({\n",
    "        \"trip_id\": test_raw[\"trip_id\"],\n",
    "        \"spend_category\": test_pred_lr.astype(int)\n",
    "    })\n",
    "    submission_lr.to_csv(\"submission_lr.csv\", index=False)\n",
    "    print(\"submission_lr.csv saved.\")\n",
    "\n",
    "    submission_svm = pd.DataFrame({\n",
    "        \"trip_id\": test_raw[\"trip_id\"],\n",
    "        \"spend_category\": test_pred_svm.astype(int)\n",
    "    })\n",
    "    submission_svm.to_csv(\"submission_svm.csv\", index=False)\n",
    "    print(\"submission_svm.csv saved.\")\n",
    "\n",
    "    submission_xgb = pd.DataFrame({\n",
    "        \"trip_id\": test_raw[\"trip_id\"],\n",
    "        \"spend_category\": test_pred_xgb.astype(int)\n",
    "    })\n",
    "    submission_xgb.to_csv(\"submission_xgb.csv\", index=False)\n",
    "    print(\"submission_xgb.csv saved.\")\n",
    "\n",
    "    submission_lgbm = pd.DataFrame({\n",
    "        \"trip_id\": test_raw[\"trip_id\"],\n",
    "        \"spend_category\": test_pred_lgbm.astype(int)\n",
    "    })\n",
    "    submission_lgbm.to_csv(\"submission_lgbm.csv\", index=False)\n",
    "    print(\"submission_lgbm.csv saved.\")\n",
    "\n",
    "    submission_cbt = pd.DataFrame({\n",
    "    # Use the raw_test_id created during CatBoost prep\n",
    "    \"trip_id\": test_raw[\"trip_id\"], \n",
    "    \"spend_category\": test_pred_cbt_flat.astype(int)\n",
    "    })\n",
    "    submission_cbt.to_csv(\"submission_cbt.csv\", index=False)\n",
    "    print(\"submission_cbt.csv saved.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "mlpro_env (3.11.4)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
